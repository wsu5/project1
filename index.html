<!doctype html>
<html>
  <head>
    <!-- Page setup -->
    <meta charset="utf-8">
    <title>Computer Vision with Animals </title>
    <meta name="description" content="Computer Vision with Animals">
    <meta name="author" content="project1 csueb">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no"/>
  
    <!-- Stylesheets -->
    <!-- Reset default styles and add support for google fonts -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" rel="stylesheet" type="text/css" />
    <link href="http://fonts.googleapis.com/css?family=Roboto" rel="stylesheet" type="text/css" />
   
    <!-- Custom styles -->
    <link href="style.css" rel="stylesheet" type="text/css" />

    <!-- jQuery -->
    <script src="https://code.jquery.com/jquery-3.4.1.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>    

    <!-- Want to add Bootstrap? -->
    <!-- Visit: https://getbootstrap.com/docs/4.3/getting-started/introduction/ -->
    
  </head>
  
  <body>

    <header id="header">
      <h1>Computer Vision and Animals</h1>
      
      <!-- Menu link fragment #id should match a div id. Example: <a href="#home"> links to <div id="home"></div>  -->
      <ul class="main-menu">
        <li><a href="#home">Home</a></li>
        <li><a href="#intro">Introduction</a></li>
        <li><a href="#sensor">Sensors</a></li>
        <li><a href="#recog">Recognition</a></li>
        <li><a href="#prediction">Prediction</a></li>
        <li><a href="#result">Result</a></li>
      </ul>                 
    </header>
   
    <div id="container">
      <div class="inner">
        <div id="content"> 
          
          <div id="home" class="content-region hide">
            <h2>Home</h2>
            <p>
              This is a simple website template published with Github Pages. <a href="https://github.com/cpl-makerlab/simple-website-template">Fork this repository</a> to create your own site!
            </p>
          </div>
          
          <div id="intro" class="content-region hide">
            <h2>Introduction</h2>
            <p>
              Animals have been a difficult object to avoid on road for human drivers. Recognizing and avoiding vehicle collisions with animals have become a greater challenge to autonomous vehicles. Comparing to human being, some animals are smaller in size which are harder to be detected and their behaviors are less predictable than human. According to Wildlife-vehicle collisions (WVCs), every year United States has 725,000 to 1,500,000 cases of vehicle collisions with wild life. And Canada has roadkill rate of 4 to 8 animals per day. By using Convolution Neural Network (CNN) to perform image segmentation and recognition, the algorithm can detect commonly found animals. Along side with lane detection and animal behavior predictions, the system has the capability to track animals’ behavior in real time and provide movement predictions. If the animal is within the predicted line, the system responses with ‘STOP’ to alert the vehicle.  
            </p>
          </div>
          
          <div id="sensor" class="content-region hide">
            <h2>Picking the Right Sensors</h2>
            <p>
              There are two common types of sensors used on autonomous vehicles to visualize surrounding environments to avoid vehicle collisions with human, animals, and obstacles. From here we compare the pros and cons in terms of performance in various conditions for LiDAR (Light Detection and Ranging) sensor and camera sensor. 
            </p>
              
            <p>
            <h3>LiDAR</h3>
              Determines ranges by targeting objects or surfaces with multiple laser beams. By measuring the time for the laser to return to the receiver, it can detect all surfaces and objects around the LiDAR. Typical LiDAR with 360 degree point of view has a scanning frequency of 10 Hz which scans the surrounding area 10 times in a second.
            </p>
          
            <p>
              <img src="lidar1.png" alt="lidar img1" width="750" height="1200">    
            </p>
          
            <p>
            <h3>Camera</h3>
              Camera sensor converts variable attenuation of light waves through lens into small bursts of current that carry the information of an image. The optical characteristics of a camera can be illustrated as below where light waves pass through camera lens at certain focal length to produce an image on the camera sensor.
              <br>
              <img src="Picture2.png" alt="cam img0">           
            </p>
        
            <p>         
            <h2>Sensor Comparison</h2>
            <h3>Camera vs LiDAR</h3>
              <p>
                LiDAR Pros: The significant advantage of LiDAR is that it can provide a 3D output even through shadows, sunlight or headlights from incoming cars which performs better than cameras or even human eyes in these harsh conditions.
              </p>
              <p>
                LiDAR cons: LiDAR can be sensitive to weather and temperature. The wavelength stability can be affected by temperature and cause a poor Signal-to-Noise ratio which potential can cause a creation of false outcomes.
              </p>
              <p>
                Camera Pros: With the combination of computer vision and machine learning models, one of the very important things that only camera can do out of all sensors is to read specific text from road signs. Cameras can also be adjustable by simply replace lens to fit different needs. For example fish eye lens can be used for object detection around the vehicle with wider field of view and telephoto lens can be used for narrow long range detection.
              </p>
              <p>
                Camera Cons: Although cameras are better at imaging, they do not have the direct feature of range detection that LiDAR has. Weather conditions can also be one of the challenges to camera. The picture below shows the camera performance could be reduced with rain or dirt on the lens.
              </p>
              <p>
                <img src="0_nzPbenQZ-y4banbT.png" alt="cam img1" width="750" height="400">
              </p>
            <h3>In Conclusion</h3>
              <p>
                <img src="1_qphKAYLL6TqRe0gwp95GLw.gif" alt="cam img1" width="750" height="400">
              </p>  
              <p>
                Since both LiDAR and camera can apply AI alogirhtms and neural networks to post process collected data. With enough computational power, a combination as sensor fusion is currently the best option for object detections with using computer vision. By combining the strength of each sensor, the final result will ensure the most optimal outcome and provide highest safety potentials. In this tutorial, we will be mostly focusing on using only camera sensor with computer vision to analyze animal categories.  
              </p>
          </div>
          
          <div id="recog" class="content-region hide">
            <h2>Animal Detection Approach</h2>
            <p>
              First phase is to select a Neural Network model that can help us with a classification problem. In this tutorial we select Mask R-CNN (Region-based Convolutional Neural Networks) model which can understand complex concepts by processing multiple layers of features. One of the key reasons for selecting Mask R-CNN is that it has already been trained on the MS COCO dataset. The dataset contains 91 common object categories and each one of them having 5,000 labeled instances. The selection of Mask R-CNN has covered the categories of cat, dog, horse, sheep, cow, elephant, bear, zebra, and giraffe.  
            </p>
            <p>
              <img src="cnn1.png" alt="cnn img1">
            </p>
            <p>
              The network is constructed using ResNet-101 backbone. Multiple layers are grouped and passed through kernels in the size of 1x1, 3x3 and 1x1 in such ordering. A batch normalization layer and ReLU activation unit follows each convolution layer. Bottom-up pathway layers are passed through 1x1 convolution layer for down sampling. Pyramid feature map are generated by having each feature map passing through the 3x3 convolution layer. Once the feature maps are generated, the outputs are fed to box detection and generate region proposals. Then the outputs are fed to box classification, regression, and mask branch. The overall architecture of Mask R-CNN results with outputs containing class IDs, bounding boxes, detection scores and the generated masks for a video frame.
            </p>
            
            <h2>Lane Detection</h2>
            <p>
              Second phase is to choose the correct methodology for lane detection. This application helps determine whether the animals post a threat to a moving vehicle. If the animals are detected to be on the side of the road, lane detection system should be able to distinguish such case to ease of reasonable amounts of concerns. Otherwise, animal directions analysis and vicinity tracking need to be performed to predict if the animal has potential intention of entering vehicle’s lane and possibly result a collision.  
            </p>
            <p>
              More detail of lane detection:
            </p>
            <p>
              The lane detection system is proposed by Param Aggarwal that uses OpenCV to detect lanes on a highway. The system first captures a picture and apply edge detection to generate results (c) from the figure below. Then it selects a region of interest which reduces unnecessary information and generates result (d). Hough transform is then used to analyze and visualize the lanes and overlap the result with highway images. At the end, to eliminate lane jittering from previous results, a moving average is applied to smooth out and stable lane detection on both sides. The detail tutorial from Aggarwal himself can be found at
              <a href="https://www.youtube.com/watch?v=a6pDdS6sY2E">HERE</a>
            </p>
            <p>
              <img src="lanedet1.png" alt="landet img1">
            </p>
     
          </div>
      
          <div id="prediction" class="content-region hide">
            <h2>Tracking</h2>
            <h3><Prediction></h3>
            <p>
              The overview structure of the prediction pipeline is combined with the Mask R-CNN model results and lane detection output to generate animal detection alert for the vehicle. If the animal falls within the detected lane, the system will send out a ‘STOP’ warning. Otherwise, it will continuously keep track of the animal’s direction of their movements.
            </p>
            <p>
              <img src="predict1.png" alt="pre1 img1">
            </p>
            <p>
              The location of the bounding box is compared with the midpoint of the lane (yellow line above) to distinguish whether the animal is within the danger zone.  
            </p>
            
            <h3>Centroid Tracking Algorithm</h3>
            <p>
              This algorithm paves the way to track animal movements over future time frames. Calculations are done by taking Euclidean distance between the centroids of a determined animal over successive frames as clarified ahead. In this system, we track the trajectories of the animal movements by calculating the differences between centroids of the animal within five consecutive frames. Since an image is in 2-dimensional space, the result can be represented as below where µ represents the direction of the animal movements. If the magnitude is over a certain pre-defined threshold, the vector µ is then normalized and stored in a dictionary for each detected animal.
              <img src="mag1.png" alt="pre1 img1">
            </p>
            
            
          </div>
          
          <div id="result" class="content-region hide">
            <h2>Experiement Setup</h2>
            <p>
              The proposed frame work was tested by MS COCO data set (Microsoft Common Objects in Context). The set contains 91 classes with 5000 annotations. Result will be evaluated in two stages: first stage of success is evaluated by the capability of distinguishing animal classes and recognizing animals from unknown object using CNN. Second stage is evaluated by the precision of predicting whether the animal has caused any potential danger to the vehicle. 
            </p>
            
            <p>
              To validate first stage, we first train the Mask R-CNN network with MS COCO dataset, then test the framework on compiled dashcam videos from YouTube taken at 30 frames per second. And these testing footages include various countries such as the USA, India, and Australia. 
            </p>
            
            <p>
              <img src="r1.png" alt="r1 img1">
            </p>
            
            <p>
              Two parameters precision and recalls are defined to summarize the result. Precision represents if the model can distinguishes animals from non-animals, we get a high precision score. Recall represents if the model detects the animal category correctly, we get a high recall score. The interpolated precision is denoted as r at each recall level. We then select the greatest precision measured as shown in Equation 4 and generate precision-recall curves from these values to calculate average precision in Equation 5.
            </p>
            
            <p>
              <img src="r2.png" alt="r1 img1">
            </p>
            
            <p>
              Results are then compared between the predicted bounding boxes generated by Mask R-CNN model and the actual bounding boxes from the data set. Intersection over Union (IoU) parameter is used to check the overlapping relationship between the predicted bounding boxes and actual bounding boxes. It’s calculated by dividing the intersectional area by the area of the union of predicted bounding boxes and actual bounding boxes.
            </p>
            
            <p>
              By repeating the same process, a precision over recall graph can be generated.
            </p>
            
            <p>
              <img src="r3.png" alt="r1 img1">
            </p>
            
            <p>
              The result shows that the test results output of Mask R-CNN model performance on test set consisting cows on road with an IoU of 0.5 has an average precision of 79.47% and dog with 81.09%
            </p>
            
            <p>
              <img src="r4.png" alt="r1 img1">
            </p>
            
          </div>
          
        </div>
      </div>
    </div>

    <footer>  
      Footer region
    </footer>
    
    <!-- Load additional JS scripts here -->
    <script type="text/javascript" src="script.js"></script>
    
  </body>
</html>
